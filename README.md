# Semantic-image-segmentation

A global rise in the occurrences of natural disasters and human-borne conflicts has put a spotlight on the need for Earth Observation (EO) data in designing practical Humanitarian Assistance and Disaster Relief (HADR) interventions. Novel techniques that leverage remotely sensed data are leading to a paradigm shift in our understanding of such situations and improving the efficacy of our response. Aerial flood maps can provide localized insight into the extent of flood-related damage and the degree to which communitiesâ€™ access to shelter, clean water, and communication channels have been compromised. Unfortunately, such insights typically only emerge hours or days after a flooding event has occurred. Moreover, a scarcity of available historical data restricts the development of practical machine learning-based methods. This work examines the use of Generative Adversarial Networks (GANs) in simulating flooding in aerial images. We first introduce the Houston UAV dataset, an extension of the FloodNet dataset. Our dataset accommodates more well-defined semantic classes and significantly reduces the label noise in semantic masks. We propose a GAN-based pipeline to generate flood conditions in non-flooded regions, generating synthetic flooding scenes for predictive mapping.

This dataset contains 374 images containing flood and non-flood scenarios. It has slightly fewer images as the FloodNet has many redundant images in the form of successive frames captured within a few milliseconds of the exact location. To keep annotation inexpensive, we choose distinct scenes and annotate them.

Proposed FloodGAN architecture:
![FloodGAN_architecture](https://user-images.githubusercontent.com/116564531/229504126-34220644-ca61-46ec-ad9c-b3092e148950.png)
*Training Conditional GAN Pix2Pix to generate a new image from a given semantic mask. UNet and Patch-GAN are used as the generator and discriminator networks, respectively.*
