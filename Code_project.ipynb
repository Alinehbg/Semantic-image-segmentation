{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7217e454fdef4d39ae5fe83565ca8302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a2aae882e1147d6ad7012afdd4505a1",
              "IPY_MODEL_5781940ef2d74778aa10d8db2ee8e6a9",
              "IPY_MODEL_7d87ddd77d0f4d48bb73c9e1ed5e4616"
            ],
            "layout": "IPY_MODEL_bf99636d13c14f2282f6fb894fc32b4f"
          }
        },
        "6a2aae882e1147d6ad7012afdd4505a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37c9e0c0e6a64f38a5bcf6db9ddea992",
            "placeholder": "​",
            "style": "IPY_MODEL_a486d1e9be0e474091e77938a5c62b92",
            "value": "100%"
          }
        },
        "5781940ef2d74778aa10d8db2ee8e6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19d6f2067bb948768f70369ca3306243",
            "max": 87306240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d70ccbd84b74f559951316f839ce992",
            "value": 87306240
          }
        },
        "7d87ddd77d0f4d48bb73c9e1ed5e4616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e9f7ae666eb470e92527b93867db924",
            "placeholder": "​",
            "style": "IPY_MODEL_bb758789b7f44975aca787a67e19d95e",
            "value": " 83.3M/83.3M [00:00&lt;00:00, 102MB/s]"
          }
        },
        "bf99636d13c14f2282f6fb894fc32b4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37c9e0c0e6a64f38a5bcf6db9ddea992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a486d1e9be0e474091e77938a5c62b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19d6f2067bb948768f70369ca3306243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d70ccbd84b74f559951316f839ce992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e9f7ae666eb470e92527b93867db924": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb758789b7f44975aca787a67e19d95e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ee2cf1ec8124d1d8b1bcc86c668d9d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_955a005b1e1443338d2204b34d296941",
              "IPY_MODEL_9df6a64cac9d46b2a977a58342cfe99f",
              "IPY_MODEL_6982a6e8b2814e2a8bff8a193ed08c44"
            ],
            "layout": "IPY_MODEL_8618b47642c34001a12540dac69ddc84"
          }
        },
        "955a005b1e1443338d2204b34d296941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b355025aeecc4fef863acd7b2331655d",
            "placeholder": "​",
            "style": "IPY_MODEL_b3ca154436c24e8e9d70b8218042a1b0",
            "value": "100%"
          }
        },
        "9df6a64cac9d46b2a977a58342cfe99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef8e23f6bf4245d5a10a120a1eb97957",
            "max": 178728960,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3311f0e0d6be4ad891deb56a6dd84192",
            "value": 178728960
          }
        },
        "6982a6e8b2814e2a8bff8a193ed08c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edb99aa3898d4155836572062c502a48",
            "placeholder": "​",
            "style": "IPY_MODEL_f5cf612f101a494497e78fab223c9b25",
            "value": " 170M/170M [00:01&lt;00:00, 104MB/s]"
          }
        },
        "8618b47642c34001a12540dac69ddc84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b355025aeecc4fef863acd7b2331655d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ca154436c24e8e9d70b8218042a1b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef8e23f6bf4245d5a10a120a1eb97957": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3311f0e0d6be4ad891deb56a6dd84192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "edb99aa3898d4155836572062c502a48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5cf612f101a494497e78fab223c9b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hvdNA_F5g_o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import glob, shutil\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "import imageio\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd3cSSfe3hru",
        "outputId": "1d3ec893-7cfd-407f-850d-5288342df47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "!mkdir Hurricane_Harvey \n",
        "!mkdir Hurricane_Harvey/rasters Hurricane_Harvey/vectors \n",
        "!gsutil -m cp -n -r gs://geoengine-dataset-houston-uav/rasters/raw Hurricane_Harvey/rasters/ \n",
        "!gsutil -m cp -n -r gs://geoengine-dataset-houston-uav/vectors/random-split-_2022_11_17-22_35_45/ Hurricane_Harvey/vectors/"
      ],
      "metadata": {
        "id": "_xDi-RrK3kIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save data in a dataframe and take a look\n",
        "# df_train = pd.read_csv('/content/Hurricane_Harvey/vectors/random-split-_2022_11_17-22_35_45/CSV/train.csv', index_col = 0)\n",
        "# Let's see the labels\n",
        "# df_train['label'].value_counts()\n",
        "# Let's see how many labels there are\n",
        "# len(df_train['label'].value_counts())\n",
        "# you lied in the description, there are more than 25 there are 27!!!!\n",
        "# The info get automaticaly saved in a raw and in masks folder, so let's load the path\n",
        "images_path = '/content/Hurricane_Harvey/rasters/raw'\n",
        "masks_path = '/content/Hurricane_Harvey/vectors/random-split-_2022_11_17-22_35_45/Masks'\n",
        "\n",
        "# Let's check we have the right amount of pictures\n",
        "images_filenames = list(sorted(os.listdir(images_path)))\n",
        "masks_filenames=list(sorted(os.listdir(masks_path)))\n",
        "# We should have 374\n",
        "print(len(images_filenames))\n",
        "# We should have 299\n",
        "print(len(masks_filenames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3FRr_WT3kDo",
        "outputId": "04ffe2c0-28fa-45e3-fe4d-7107e02c885a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "374\n",
            "299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We are going to move the images from a local folder to a folder in Drive, so here we set the folders\n",
        "# This is for the trainig images (the ones that also have masks)\n",
        "entrena_imagenes='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Entrena_fotos_1'\n",
        "entrena_masks='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Entrena_masks_1'\n",
        "valida_imagenes='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Testing_fotos_1'"
      ],
      "metadata": {
        "id": "RHIMZIF03j_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an auxiliary list with the name of the masks, with no termination\n",
        "masks_filenames_no_termi=[]\n",
        "for nom in masks_filenames:\n",
        "  # Extract the name without the .png termination\n",
        "  masks_filenames_no_termi.append(nom[:-4])\n",
        "# Let's take a look\n",
        "masks_filenames_no_termi[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjrkPZ-J3hnR",
        "outputId": "f7b3c2dc-fc7a-494b-f378-c5d5f08fc1a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['10170', '10171', '10172']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we move the files to their corresponding folder\n",
        "for dir in images_filenames:\n",
        "  if dir[:-4] in masks_filenames_no_termi:\n",
        "    # the png are the masks\n",
        "    shutil.copy2(masks_path+'/'+dir[:-4]+'.png', entrena_masks) #put the masks in the train folder\n",
        "    # the tif are the imges\n",
        "    shutil.copy2(images_path+'/'+dir[:-4]+'.tif', entrena_imagenes) #put the images in the train folder\n",
        "  else:\n",
        "    # these are the images for the validation\n",
        "    shutil.copy2(images_path+'/'+dir[:-4]+'.tif', valida_imagenes) #put just the img in the validation folder\n",
        "\n",
        "# Check if it did the job\n",
        "# the images (they have to be 299)\n",
        "print(len(list(sorted(os.listdir(entrena_imagenes)))))\n",
        "# the labels (they have to be 299)\n",
        "print(len(list(sorted(os.listdir(entrena_masks)))))\n",
        "# the ones we'll have to predict (they have to be 75=374-299)\n",
        "print(len(list(sorted(os.listdir(valida_imagenes)))))\n",
        "#  Looks good"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd7JPxoq3hcZ",
        "outputId": "d4543dec-3696-46ef-e6d1-2c97cf23fe4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "299\n",
            "299\n",
            "75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install patchify\n",
        "import cv2\n",
        "from patchify import patchify\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "scaler = MinMaxScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcU9BhU33qzN",
        "outputId": "ef328f74-91c5-4a76-9bde-8ab15e70821a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting patchify\n",
            "  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.8/dist-packages (from patchify) (1.21.6)\n",
            "Installing collected packages: patchify\n",
            "Successfully installed patchify-0.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alto=256\n",
        "ancho=256\n",
        "# imagenes_transf= transforms.Compose([transforms.ToTensor()])\n",
        "imagenes_transf= transforms.Compose([transforms.Resize([alto,alto]),transforms.ToTensor()])\n",
        "masks_transf=transforms.Compose([transforms.Resize([alto,alto], interpolation=T.InterpolationMode.BILINEAR), transforms.ToTensor()])"
      ],
      "metadata": {
        "id": "V6Qh6Y7q3qvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select images to make an augmentation\n",
        "# we are going to select 45 images to make them a transformation\n",
        "import random\n",
        "# masks_filenames_no_termi\n",
        "rotacion=random.sample(masks_filenames_no_termi, 15)\n",
        "# main_list = list(set(list_2) - set(list_1))\n",
        "flip_horizontal=random.sample(list(set(masks_filenames_no_termi)-set(rotacion)),15)\n",
        "# list(set(masks_filenames_no_termi)-set(rotacion))\n",
        "restante=list(set(masks_filenames_no_termi)-set(rotacion))\n",
        "flip_vertical=random.sample(list(set(restante)-set(flip_horizontal)),15)"
      ],
      "metadata": {
        "id": "-J2Jv3snZypN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# patch_size=256\n",
        "# image_dataset = []\n",
        "# for foto in masks_filenames_no_termi:\n",
        "tensor_image_path='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Image_tensors'\n",
        "tensor_mask_path='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Masks_tensors'\n",
        "for foto in masks_filenames_no_termi:\n",
        "\n",
        "  # open and transform the images\n",
        "  abre_imagen=imagenes_transf(Image.open(entrena_imagenes+'/'+foto+'.tif'))\n",
        "  abre_mask=imagenes_transf(Image.open(entrena_masks+'/'+foto+'.png'))*255\n",
        "  abre_mask=abre_mask.int()\n",
        "\n",
        "  # save them as tensors\n",
        "  torch.save(abre_imagen, tensor_image_path+'/'+foto+'.pt')\n",
        "  torch.save(abre_mask, tensor_mask_path+'/'+foto+'.pt')\n",
        "\n",
        "  # save transformations as tensors also\n",
        "  if foto in rotacion:\n",
        "    t_rotacion = T.RandomRotation(degrees=(0, 180))\n",
        "    imagen_rotada= t_rotacion(abre_imagen)\n",
        "    mask_rotada= t_rotacion(abre_mask)\n",
        "\n",
        "    # save the rotated images and masks with a different name\n",
        "    torch.save(imagen_rotada, tensor_image_path+'/'+foto+'_rotada.pt')\n",
        "    torch.save(mask_rotada, tensor_mask_path+'/'+foto+'_rotada.pt')\n",
        "  \n",
        "  # save horizontal flips\n",
        "  elif foto in flip_horizontal:\n",
        "    t_horizontal=T.RandomHorizontalFlip(p=1)\n",
        "    imagen_hori = t_horizontal(abre_imagen)\n",
        "    mask_hori = t_horizontal(abre_mask)\n",
        "\n",
        "    # save the horizontal images and masks with a different name\n",
        "    torch.save(imagen_hori, tensor_image_path+'/'+foto+'_hori.pt')\n",
        "    torch.save(mask_hori, tensor_mask_path+'/'+foto+'_hori.pt')\n",
        "  \n",
        "  # save vertical flips\n",
        "  elif foto in flip_vertical:\n",
        "    t_vertical=T.RandomVerticalFlip(p=1)\n",
        "    imagen_verti=t_vertical(abre_imagen)\n",
        "    mask_verti=t_vertical(abre_mask)\n",
        "\n",
        "    # save the vertical images and masks with a different name\n",
        "    torch.save(imagen_verti, tensor_image_path+'/'+foto+'_verti.pt')\n",
        "    torch.save(mask_verti, tensor_mask_path+'/'+foto+'_verti.pt')\n",
        "  \n",
        "  else:\n",
        "    pass"
      ],
      "metadata": {
        "id": "B_q41ya663rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_image_path='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Image_tensors'\n",
        "tensor_mask_path='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Masks_tensors'"
      ],
      "metadata": {
        "id": "pun_vLEaKr5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# they should be 344 = 299+45 (15 rotated, 15 flipped horizontally, 15 flipped vertically)\n",
        "print(len(sorted(glob.glob(tensor_image_path+'/*'))))\n",
        "print(len(sorted(glob.glob(tensor_mask_path+'/*'))))"
      ],
      "metadata": {
        "id": "1MxSl-LbEelB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we have to make our selection of images for the validation and the training dataset\n",
        "# make the sample of fotos for trianing and validation\n",
        "fotos_entrena, fotos_valida=train_test_split(sorted(os.listdir(tensor_image_path)), test_size=0.10, random_state=13)"
      ],
      "metadata": {
        "id": "uOpIHzyKFnp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(fotos_valida)"
      ],
      "metadata": {
        "id": "5361Z6p9RNZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "tensor_image_path='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Image_tensors'\n",
        "tensor_mask_path='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Masks_tensors'"
      ],
      "metadata": {
        "id": "DqpSf_3nKcev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder for training\n",
        "tensor_image_train='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Tensor_entrenamiento/Tensor_entrenamiento_images'\n",
        "tensor_mask_train='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Tensor_entrenamiento/Tensor_entrenamiento_masks'\n",
        "\n",
        "# Folde for validation\n",
        "tensor_image_val='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Tensor_validacion/Tensor_validacion_images'\n",
        "tensor_mask_val='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Tensor_validacion/Tensor_validacion_masks'"
      ],
      "metadata": {
        "id": "m-ChcTFLNmRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for archivo in sorted(os.listdir(tensor_image_path)):\n",
        "  # print(tensor_image_path+'/'+archivo)\n",
        "  if archivo in fotos_entrena:\n",
        "    \n",
        "    # copy image for training\n",
        "    shutil.copyfile(tensor_image_path+'/'+archivo,tensor_image_train+'/'+archivo)\n",
        "    # copy mask for training\n",
        "    shutil.copyfile(tensor_mask_path+'/'+archivo,tensor_mask_train+'/'+archivo)\n",
        "  \n",
        "  # the else is for the tensor that are not in the training, so they are in validation\n",
        "  else:\n",
        "\n",
        "    # copy image for validation\n",
        "    shutil.copyfile(tensor_image_path+'/'+archivo,tensor_image_val+'/'+archivo)\n",
        "    # copy mask for validation\n",
        "    shutil.copyfile(tensor_mask_path+'/'+archivo,tensor_mask_val+'/'+archivo)\n",
        "  "
      ],
      "metadata": {
        "id": "ARlCyIz0KcXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if we have the images in the paths\n",
        "\n",
        "#  258\n",
        "print(len(os.listdir(tensor_image_train)))\n",
        "print(len(os.listdir(tensor_mask_train)))\n",
        "# 86\n",
        "print(len(os.listdir(tensor_image_val)))\n",
        "print(len(os.listdir(tensor_mask_val)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KTmzyA6KcSH",
        "outputId": "b307f3ed-0ef2-44f6-c237-51d785f1f6e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "258\n",
            "258\n",
            "86\n",
            "86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loaders"
      ],
      "metadata": {
        "id": "y30HM8sqTcTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom Dataset class\n",
        "class MiTensorDataset(Dataset):\n",
        "       \n",
        "    def __init__(self, images:list, masks:list):\n",
        "        self.image_links = images\n",
        "        self.mask_links  = masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_links) \n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        # Select a specific image's link\n",
        "        img_id  = self.image_links[index]\n",
        "        mask_id = self.mask_links[index]\n",
        "\n",
        "        # Load the image (as tensor)\n",
        "        img  = torch.load(img_id)\n",
        "        mask = torch.load(mask_id)\n",
        "                      \n",
        "        # Squeeze mask from (1, H, W) to (H, W) -> This is because loss function accepts (B, H, W)\n",
        "        mask = mask.squeeze(0)\n",
        "\n",
        "        # Turn on gradient for image\n",
        "        img = img.detach().clone().requires_grad_(True)\n",
        "        mask = mask.long()\n",
        "        \n",
        "        return img, mask\n",
        "    "
      ],
      "metadata": {
        "id": "i3qMZyh3TJIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder for training\n",
        "tensor_image_train=sorted(glob.glob('/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Tensor_entrenamiento/Tensor_entrenamiento_images/*'))\n",
        "tensor_mask_train=sorted(glob.glob('/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Tensor_entrenamiento/Tensor_entrenamiento_masks/*'))\n",
        "\n",
        "# Folder for validation\n",
        "tensor_image_val=sorted(glob.glob('/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Tensor_validacion/Tensor_validacion_images/*'))\n",
        "tensor_mask_val=sorted(glob.glob('/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/Fotos_1/Tensors/Tensor_validacion/Tensor_validacion_masks/*'))"
      ],
      "metadata": {
        "id": "HpbckYDl3qkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_image_train"
      ],
      "metadata": {
        "id": "A8B_4qHP8D2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_entrenamiento=MiTensorDataset(tensor_image_train,tensor_mask_train)\n",
        "dataloader_entrenamiento=DataLoader(dataset_entrenamiento, batch_size=10, shuffle=10)\n",
        "dataset_val=MiTensorDataset(tensor_image_val,tensor_mask_val)\n",
        "dataloader_val=DataLoader(dataset_val, batch_size=10, shuffle=10)"
      ],
      "metadata": {
        "id": "olZh8RFV2_9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(dataloader_entrenamiento):\n",
        "    img_batch, img_mask = batch\n",
        "    print(img_batch.shape)\n",
        "    print(img_mask.shape)"
      ],
      "metadata": {
        "id": "tWfnX2J4TJEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNET"
      ],
      "metadata": {
        "id": "iBvCMwaC34vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.functional as TF\n",
        "# based on the paper\n",
        "# https://arxiv.org/abs/1505.04597\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(\n",
        "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
        "    ):\n",
        "        super(UNET, self).__init__()\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Down part of UNET\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(in_channels, feature))\n",
        "            in_channels = feature\n",
        "\n",
        "        # Up part of UNET\n",
        "        for feature in reversed(features):\n",
        "            self.ups.append(\n",
        "                nn.ConvTranspose2d(\n",
        "                    feature*2, feature, kernel_size=2, stride=2,\n",
        "                )\n",
        "            )\n",
        "            self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
        "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for idx in range(0, len(self.ups), 2):\n",
        "            x = self.ups[idx](x)\n",
        "            skip_connection = skip_connections[idx//2]\n",
        "\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "            x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "iheifsZ_TJA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if the UNET works\n",
        "x = torch.randn((3, 2, 161, 161))\n",
        "model = UNET(in_channels=2, out_channels=9)\n",
        "preds = model(x)"
      ],
      "metadata": {
        "id": "8mkKJ8r2TI9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define device\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OGPpGRg5Jkh",
        "outputId": "cfda4c0f-3473-47f6-9e30-75c6a66dcdc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNEt on the data\n",
        "model = UNET(in_channels=3, out_channels=27).to(device)"
      ],
      "metadata": {
        "id": "WwkbQKtp5Sx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alto=256\n",
        "ancho=256\n",
        "# Accuracy\n",
        "def model_accuracy(predicciones, mask_correcta):#preds, true_mask):\n",
        "  # this function runs over all the batch, so we have to create a list to save all the accuracy points por each\n",
        "  # of the images and masks\n",
        "    lista_accuracy_results= []# initialize list\n",
        "    # iterate through the predictions\n",
        "    for pred in range(len(predicciones)):\n",
        "        # img  = preds[i].to(device)\n",
        "        # mask = true_mask[i].to(device)\n",
        "        foto = predicciones[pred]\n",
        "        mask = mask_correcta[pred]\n",
        "        \n",
        "        # here we change the shape of the image, because is has 3 dimensions and we want it to match\n",
        "        # the dimensions of the mask\n",
        "        foto = torch.argmax(foto, dim=0)\n",
        "        # we calculate the accuracy\n",
        "        acc_num=100*torch.sum(foto == mask).item() / (alto*ancho)\n",
        "        # we append it to the list\n",
        "        lista_accuracy_results.append(acc_num)  \n",
        "        \n",
        "    return np.mean(lista_accuracy_results)"
      ],
      "metadata": {
        "id": "FongJhmn5SfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate the Dice score\n",
        "def DiceScore(predicciones, mascara_correcta):\n",
        "    dice_batch = []\n",
        "\n",
        "    for i in range(len(predicciones)):\n",
        "        dice_image = []\n",
        "        img  = predicciones[i].to(device)\n",
        "        mask = mascara_correcta[i].to(device)\n",
        "        # img  = predicciones[i]\n",
        "        # mask = mascara_correcta[i]\n",
        "        \n",
        "        img = torch.argmax(img, dim=0)\n",
        "    \n",
        "        for label in range(27):\n",
        "            if torch.sum(mask == label) != 0:\n",
        "                area_of_intersect = torch.sum((img == label) * (mask == label))\n",
        "                area_of_img       = torch.sum(img == label)\n",
        "                area_of_label     = torch.sum(mask == label)\n",
        "                dice = 2*area_of_intersect / (area_of_img + area_of_label)\n",
        "                dice_image.append(dice)\n",
        "        \n",
        "        dice_batch.append(np.mean([tensor.cpu() for tensor in dice_image]))\n",
        "    return np.mean(dice_batch)\n"
      ],
      "metadata": {
        "id": "UH5eGBUECwwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "total_train_losses   = []\n",
        "total_train_accuracy=[]\n",
        "total_val_losses=[]\n",
        "total_val_accuracy=[]\n",
        "total_train_dice=[]\n",
        "total_val_dice=[]"
      ],
      "metadata": {
        "id": "SL5qkktD5St1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_res='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/ResUNET_mano/Primero'\n",
        "for epoch in range(1, epochs+1):\n",
        "    \n",
        "    # Train model\n",
        "    model.train()\n",
        "    train_losses   = []\n",
        "    train_accuracy = []\n",
        "    train_dice       = []\n",
        "    \n",
        "    for i, batch in enumerate(dataloader_val):\n",
        "        \n",
        "        img_batch, mask_batch = batch   \n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "\n",
        "        #Train model\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(img_batch)\n",
        "        loss   = criterion(output, mask_batch)\n",
        "        loss.backward()\n",
        "\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 6)\n",
        "        optimizer.step()\n",
        "\n",
        "        \n",
        "        dice = DiceScore(output, mask_batch)\n",
        "        acc = model_accuracy(output, mask_batch)\n",
        "        train_losses.append(loss.item())\n",
        "        train_accuracy.append(acc)\n",
        "        train_dice.append(dice)\n",
        "        \n",
        "    \n",
        "    # RESULTS\n",
        "    print(f'TRAINING')\n",
        "    print(f'Epoch: {epoch} | Loss: {np.mean(train_losses):.4f}, Accuracy: {np.mean(train_accuracy):.3f}, Dice: {np.mean(train_dice):.3f}')        \n",
        "    total_train_losses.append(np.mean(train_losses))\n",
        "    total_train_accuracy.append(np.mean(train_accuracy))\n",
        "    total_train_dice.append(np.mean(train_dice))\n",
        "    \n",
        "    \n",
        "    ######################### Validation of the model\n",
        "    model.eval()\n",
        "    val_losses   = []\n",
        "    val_accuracy = []\n",
        "    val_dice       = []\n",
        "    \n",
        "    for i, batch in enumerate(dataloader_val):\n",
        "        \n",
        "        img_batch, mask_batch = batch\n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "\n",
        "        \n",
        "        with torch.cuda.amp.autocast():\n",
        "          output = model(img_batch)\n",
        "          loss   = criterion(output, mask_batch)\n",
        "\n",
        "        dice = DiceScore(output, mask_batch)\n",
        "        acc = model_accuracy(output, mask_batch)\n",
        "        val_losses.append(loss.item())\n",
        "        val_accuracy.append(acc)\n",
        "        val_dice.append(dice)\n",
        "        \n",
        "    \n",
        "    # Results\n",
        "    print(f'VALIDATION')\n",
        "    print(f'Epoch: {epoch} | Loss: {np.mean(val_losses):.4f}, Accuracy: {np.mean(val_accuracy):.3f}, Dice: {np.mean(val_dice):.3f}')\n",
        "    print('########################################################################################')\n",
        "    total_val_losses.append(np.mean(val_losses))\n",
        "    total_val_accuracy.append(np.mean(val_accuracy))\n",
        "    total_val_dice.append(np.mean(val_dice))\n",
        "        \n",
        "    # torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto/Primeros_resultados_PSPNet/PSPNet_res101_368_{epoch}.pt')\n",
        "    torch.save(model.state_dict(), path_res+'/modelo_'+str(epoch)+'.pt')\n",
        "    \n",
        "    \n",
        "    \n",
        "    resultados = pd.DataFrame(list(zip(total_train_losses, total_val_losses, total_train_dice, total_val_dice,total_train_accuracy, total_val_accuracy)),\n",
        "                          columns = ['train_loss', 'val_loss', 'train_dice', 'test_dice', 'train_accuracy','val_accuracy'])\n",
        "    resultados.to_csv(path_res+'/'+'res_table.csv')\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "F6kcM6sh5Jg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(list(range(len(total_train_losses))), total_train_losses)\n",
        "plt.plot(list(range(len(total_train_losses))), total_val_losses)\n",
        "plt.legend(['Training loss', 'Validation loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DmimTOr85JSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultados.head()"
      ],
      "metadata": {
        "id": "SeIMr_sWB-dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNET 2"
      ],
      "metadata": {
        "id": "DA5lgPx5byNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "total_train_losses   = []\n",
        "total_train_accuracy=[]\n",
        "total_val_losses=[]\n",
        "total_val_accuracy=[]\n",
        "total_train_dice=[]\n",
        "total_val_dice=[]"
      ],
      "metadata": {
        "id": "MyRtR3QgbeFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_res='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/ResUNET_mano/Segundo'\n",
        "for epoch in range(1, epochs+1):\n",
        "    \n",
        "    # Train model\n",
        "    model.train()\n",
        "    train_losses   = []\n",
        "    train_accuracy = []\n",
        "    train_dice       = []\n",
        "    \n",
        "    for i, batch in enumerate(dataloader_val):\n",
        "        img_batch, mask_batch = batch   \n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "\n",
        "        #Train model\n",
        "        optimizer.zero_grad()\n",
        "        output = model(img_batch)\n",
        "        loss   = criterion(output, mask_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        dice = DiceScore(output, mask_batch)\n",
        "        acc = model_accuracy(output, mask_batch)\n",
        "        train_losses.append(loss.item())\n",
        "        train_accuracy.append(acc)\n",
        "        train_dice.append(dice)\n",
        "        \n",
        "    \n",
        "    # RESULTS\n",
        "    print(f'TRAINING')\n",
        "    print(f'Epoch: {epoch} | Loss: {np.mean(train_losses):.4f}, Accuracy: {np.mean(train_accuracy):.3f}, Dice: {np.mean(train_dice):.3f}')        \n",
        "    total_train_losses.append(np.mean(train_losses))\n",
        "    total_train_accuracy.append(np.mean(train_accuracy))\n",
        "    total_train_dice.append(np.mean(train_dice))\n",
        "    \n",
        "    \n",
        "    ######################### Validation of the model\n",
        "    model.eval()\n",
        "    val_losses   = []\n",
        "    val_accuracy = []\n",
        "    val_dice       = []\n",
        "    \n",
        "    for i, batch in enumerate(dataloader_val):\n",
        "        img_batch, mask_batch = batch\n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "          output = model(img_batch)\n",
        "          loss   = criterion(output, mask_batch)\n",
        "        dice = DiceScore(output, mask_batch)\n",
        "        acc = model_accuracy(output, mask_batch)\n",
        "        val_losses.append(loss.item())\n",
        "        val_accuracy.append(acc)\n",
        "        val_dice.append(dice)\n",
        "        \n",
        "    \n",
        "    # Results\n",
        "    print(f'VALIDATION')\n",
        "    print(f'Epoch: {epoch} | Loss: {np.mean(val_losses):.4f}, Accuracy: {np.mean(val_accuracy):.3f}, Dice: {np.mean(val_dice):.3f}')\n",
        "    print('########################################################################################')\n",
        "    total_val_losses.append(np.mean(val_losses))\n",
        "    total_val_accuracy.append(np.mean(val_accuracy))\n",
        "    total_val_dice.append(np.mean(val_dice))\n",
        "        \n",
        "    # torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto/Primeros_resultados_PSPNet/PSPNet_res101_368_{epoch}.pt')\n",
        "    torch.save(model.state_dict(), path_res+'/modelo_'+str(epoch)+'.pt')\n",
        "    resultados = pd.DataFrame(list(zip(total_train_losses, total_val_losses, total_train_dice, total_val_dice,total_train_accuracy, total_val_accuracy)),\n",
        "                          columns = ['train_loss', 'val_loss', 'train_dice', 'test_dice', 'train_accuracy','val_accuracy'])\n",
        "    resultados.to_csv(path_res+'/'+'res_table.csv')\n",
        "    \n"
      ],
      "metadata": {
        "id": "rOw-zBAbbdrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(list(range(len(total_train_losses))), total_train_losses)\n",
        "plt.plot(list(range(len(total_train_losses))), total_val_losses)\n",
        "plt.legend(['Training loss', 'Validation loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F5QB-Qb4bdnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "total_train_losses   = []\n",
        "total_train_accuracy=[]\n",
        "total_val_losses=[]\n",
        "total_val_accuracy=[]\n",
        "total_train_dice=[]\n",
        "total_val_dice=[]"
      ],
      "metadata": {
        "id": "vZluuisXbdja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_res='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/ResUNET_mano/Tercero'\n",
        "for epoch in range(1, epochs+1):\n",
        "    \n",
        "    # Train model\n",
        "    model.train()\n",
        "    train_losses   = []\n",
        "    train_accuracy = []\n",
        "    train_dice       = []\n",
        "    \n",
        "    for i, batch in enumerate(dataloader_val):\n",
        "        img_batch, mask_batch = batch   \n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "\n",
        "        #Train model\n",
        "        optimizer.zero_grad()\n",
        "        output = model(img_batch)\n",
        "        loss   = criterion(output, mask_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        dice = DiceScore(output, mask_batch)\n",
        "        acc = model_accuracy(output, mask_batch)\n",
        "        train_losses.append(loss.item())\n",
        "        train_accuracy.append(acc)\n",
        "        train_dice.append(dice)\n",
        "        \n",
        "    \n",
        "    # RESULTS\n",
        "    print(f'TRAINING')\n",
        "    print(f'Epoch: {epoch} | Loss: {np.mean(train_losses):.4f}, Accuracy: {np.mean(train_accuracy):.3f}, Dice: {np.mean(train_dice):.3f}')        \n",
        "    total_train_losses.append(np.mean(train_losses))\n",
        "    total_train_accuracy.append(np.mean(train_accuracy))\n",
        "    total_train_dice.append(np.mean(train_dice))\n",
        "    \n",
        "    \n",
        "    ######################### Validation of the model\n",
        "    model.eval()\n",
        "    val_losses   = []\n",
        "    val_accuracy = []\n",
        "    val_dice       = []\n",
        "    \n",
        "    for i, batch in enumerate(dataloader_val):\n",
        "        img_batch, mask_batch = batch\n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "\n",
        "        \n",
        "        with torch.cuda.amp.autocast():\n",
        "          output = model(img_batch)\n",
        "          loss   = criterion(output, mask_batch)\n",
        "        dice = DiceScore(output, mask_batch)\n",
        "        acc = model_accuracy(output, mask_batch)\n",
        "        val_losses.append(loss.item())\n",
        "        val_accuracy.append(acc)\n",
        "        val_dice.append(dice)\n",
        "        \n",
        "    \n",
        "    # Results\n",
        "    print(f'VALIDATION')\n",
        "    print(f'Epoch: {epoch} | Loss: {np.mean(val_losses):.4f}, Accuracy: {np.mean(val_accuracy):.3f}, Dice: {np.mean(val_dice):.3f}')\n",
        "    print('########################################################################################')\n",
        "    total_val_losses.append(np.mean(val_losses))\n",
        "    total_val_accuracy.append(np.mean(val_accuracy))\n",
        "    total_val_dice.append(np.mean(val_dice))\n",
        "        \n",
        "    # torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto/Primeros_resultados_PSPNet/PSPNet_res101_368_{epoch}.pt')\n",
        "    torch.save(model.state_dict(), path_res+'/modelo_'+str(epoch)+'.pt')\n",
        "    resultados = pd.DataFrame(list(zip(total_train_losses, total_val_losses, total_train_dice, total_val_dice,total_train_accuracy, total_val_accuracy)),\n",
        "                          columns = ['train_loss', 'val_loss', 'train_dice', 'test_dice', 'train_accuracy','val_accuracy'])\n",
        "    resultados.to_csv(path_res+'/'+'res_table.csv')"
      ],
      "metadata": {
        "id": "HxEXoxz4dMXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(list(range(len(total_train_losses))), total_train_losses)\n",
        "plt.plot(list(range(len(total_train_losses))), total_val_losses)\n",
        "plt.legend(['Training loss', 'Validation loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qccl_oFcdLJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# even though the epoch 11 has the best accuracy for this net, the dice scrore is super low"
      ],
      "metadata": {
        "id": "wxxxx08xgLoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UNET using ResNet as encoder"
      ],
      "metadata": {
        "id": "onNSOFTLgooA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\n",
        "import segmentation_models_pytorch as smp"
      ],
      "metadata": {
        "id": "aiUGELrngLjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = smp.Unet(\n",
        "    encoder_name = 'resnet34', \n",
        "    encoder_weights = 'imagenet', \n",
        "    classes = 27, \n",
        "    activation = None,\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "7217e454fdef4d39ae5fe83565ca8302",
            "6a2aae882e1147d6ad7012afdd4505a1",
            "5781940ef2d74778aa10d8db2ee8e6a9",
            "7d87ddd77d0f4d48bb73c9e1ed5e4616",
            "bf99636d13c14f2282f6fb894fc32b4f",
            "37c9e0c0e6a64f38a5bcf6db9ddea992",
            "a486d1e9be0e474091e77938a5c62b92",
            "19d6f2067bb948768f70369ca3306243",
            "8d70ccbd84b74f559951316f839ce992",
            "2e9f7ae666eb470e92527b93867db924",
            "bb758789b7f44975aca787a67e19d95e"
          ]
        },
        "id": "ZEdMkKFTgLWi",
        "outputId": "b191f413-9c21-4f78-afa9-d49ad2efdfa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/83.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7217e454fdef4d39ae5fe83565ca8302"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "learning_rate = 0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "total_train_losses   = []\n",
        "total_train_accuracy=[]\n",
        "total_val_losses=[]\n",
        "total_val_accuracy=[]\n",
        "total_train_dice=[]\n",
        "total_val_dice=[]"
      ],
      "metadata": {
        "id": "VT1ZSEGeh3_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_res='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/PSPnet_UNET/PSP_Unet_primero'\n",
        "for epoch in range(1, epochs+1):\n",
        "    \n",
        "    # Train model\n",
        "    model.train()\n",
        "    train_losses   = []\n",
        "    train_accuracy = []\n",
        "    train_dice       = []\n",
        "    \n",
        "    for i, batch in enumerate(dataloader_val):\n",
        "        img_batch, mask_batch = batch   \n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "\n",
        "        #Train model\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(img_batch)\n",
        "        loss   = criterion(output, mask_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()        \n",
        "        dice = DiceScore(output, mask_batch)\n",
        "        acc = model_accuracy(output, mask_batch)\n",
        "        train_losses.append(loss.item())\n",
        "        train_accuracy.append(acc)\n",
        "        train_dice.append(dice)\n",
        "        \n",
        "    \n",
        "    # RESULTS\n",
        "    print(f'TRAINING')\n",
        "    print(f'Epoch: {epoch} | Loss: {np.mean(train_losses):.4f}, Accuracy: {np.mean(train_accuracy):.3f}, Dice: {np.mean(train_dice):.3f}')        \n",
        "    total_train_losses.append(np.mean(train_losses))\n",
        "    total_train_accuracy.append(np.mean(train_accuracy))\n",
        "    total_train_dice.append(np.mean(train_dice))\n",
        "    \n",
        "    \n",
        "    ######################### Validation of the model\n",
        "    model.eval()\n",
        "    val_losses   = []\n",
        "    val_accuracy = []\n",
        "    val_dice       = []\n",
        "    \n",
        "    for i, batch in enumerate(dataloader_val):      \n",
        "        img_batch, mask_batch = batch\n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "          output = model(img_batch)\n",
        "          loss   = criterion(output, mask_batch)\n",
        "        dice = DiceScore(output, mask_batch)\n",
        "        acc = model_accuracy(output, mask_batch)\n",
        "        val_losses.append(loss.item())\n",
        "        val_accuracy.append(acc)\n",
        "        val_dice.append(dice)\n",
        "        \n",
        "    \n",
        "    # Results\n",
        "    print(f'VALIDATION')\n",
        "    print(f'Epoch: {epoch} | Loss: {np.mean(val_losses):.4f}, Accuracy: {np.mean(val_accuracy):.3f}, Dice: {np.mean(val_dice):.3f}')\n",
        "    print('########################################################################################')\n",
        "    total_val_losses.append(np.mean(val_losses))\n",
        "    total_val_accuracy.append(np.mean(val_accuracy))\n",
        "    total_val_dice.append(np.mean(val_dice))\n",
        "        \n",
        "    # torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto/Primeros_resultados_PSPNet/PSPNet_res101_368_{epoch}.pt')\n",
        "    torch.save(model.state_dict(), path_res+'/modelo_'+str(epoch)+'.pt')\n",
        "    resultados = pd.DataFrame(list(zip(total_train_losses, total_val_losses, total_train_dice, total_val_dice,total_train_accuracy, total_val_accuracy)),\n",
        "                          columns = ['train_loss', 'val_loss', 'train_dice', 'test_dice', 'train_accuracy','val_accuracy'])\n",
        "    resultados.to_csv(path_res+'/'+'res_table.csv')"
      ],
      "metadata": {
        "id": "qPwD2aw6h37Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(list(range(len(total_train_losses))), total_train_losses)\n",
        "plt.plot(list(range(len(total_train_losses))), total_val_losses)\n",
        "plt.legend(['Training loss', 'Validation loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cvfN7oeyh33V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PSPNet using ResNet encoder**"
      ],
      "metadata": {
        "id": "O0ctd4y6lM0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = smp.PSPNet(\n",
        "    encoder_name = 'resnet101', \n",
        "    encoder_weights = 'imagenet', \n",
        "    classes = 27, \n",
        "    activation = None, # could be None for logits or 'softmax2d' for multiclass segmentation\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "7ee2cf1ec8124d1d8b1bcc86c668d9d8",
            "955a005b1e1443338d2204b34d296941",
            "9df6a64cac9d46b2a977a58342cfe99f",
            "6982a6e8b2814e2a8bff8a193ed08c44",
            "8618b47642c34001a12540dac69ddc84",
            "b355025aeecc4fef863acd7b2331655d",
            "b3ca154436c24e8e9d70b8218042a1b0",
            "ef8e23f6bf4245d5a10a120a1eb97957",
            "3311f0e0d6be4ad891deb56a6dd84192",
            "edb99aa3898d4155836572062c502a48",
            "f5cf612f101a494497e78fab223c9b25"
          ]
        },
        "id": "oTkQ2OZzmZOJ",
        "outputId": "085e8186-0333-469e-d575-2d7d667159a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/170M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ee2cf1ec8124d1d8b1bcc86c668d9d8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "total_train_losses   = []\n",
        "total_train_accuracy=[]\n",
        "total_val_losses=[]\n",
        "total_val_accuracy=[]\n",
        "total_train_dice=[]\n",
        "total_val_dice=[]"
      ],
      "metadata": {
        "id": "3QhED4avj7TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_res='/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto_yo/PSPnet_RESnet'\n",
        "for epoch in range(1, epochs+1):\n",
        "    \n",
        "    # Train model\n",
        "    model.train()\n",
        "    train_losses   = []\n",
        "    train_accuracy = []\n",
        "    train_dice       = []  \n",
        "    for i, batch in enumerate(dataloader_val):\n",
        "        img_batch, mask_batch = batch   \n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "\n",
        "        #Train model\n",
        "        optimizer.zero_grad()     \n",
        "        output = model(img_batch)\n",
        "        loss   = criterion(output, mask_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()       \n",
        "        dice = DiceScore(output, mask_batch)\n",
        "        acc = model_accuracy(output, mask_batch)\n",
        "        train_losses.append(loss.item())\n",
        "        train_accuracy.append(acc)\n",
        "        train_dice.append(dice)\n",
        "        \n",
        "    \n",
        "    # RESULTS\n",
        "    print(f'TRAINING')\n",
        "    print(f'Epoch: {epoch} | Loss: {np.mean(train_losses):.4f}, Accuracy: {np.mean(train_accuracy):.3f}, Dice: {np.mean(train_dice):.3f}')        \n",
        "    total_train_losses.append(np.mean(train_losses))\n",
        "    total_train_accuracy.append(np.mean(train_accuracy))\n",
        "    total_train_dice.append(np.mean(train_dice))\n",
        "    \n",
        "    \n",
        "    ######################### Validation of the model\n",
        "    model.eval()\n",
        "    val_losses   = []\n",
        "    val_accuracy = []\n",
        "    val_dice       = []\n",
        "    \n",
        "    for i, batch in enumerate(dataloader_val):   \n",
        "        img_batch, mask_batch = batch\n",
        "        img_batch = img_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "      \n",
        "        with torch.cuda.amp.autocast():\n",
        "          output = model(img_batch)\n",
        "          loss   = criterion(output, mask_batch)\n",
        "\n",
        "        dice = DiceScore(output, mask_batch)\n",
        "        acc = model_accuracy(output, mask_batch)\n",
        "        val_losses.append(loss.item())\n",
        "        val_accuracy.append(acc)\n",
        "        val_dice.append(dice)\n",
        "        \n",
        "    \n",
        "    # Results\n",
        "    print(f'VALIDATION')\n",
        "    print(f'Epoch: {epoch} | Loss: {np.mean(val_losses):.4f}, Accuracy: {np.mean(val_accuracy):.3f}, Dice: {np.mean(val_dice):.3f}')\n",
        "    print('########################################################################################')\n",
        "    total_val_losses.append(np.mean(val_losses))\n",
        "    total_val_accuracy.append(np.mean(val_accuracy))\n",
        "    total_val_dice.append(np.mean(val_dice))\n",
        "        \n",
        "    # torch.save(model.state_dict(), f'/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto/Primeros_resultados_PSPNet/PSPNet_res101_368_{epoch}.pt')\n",
        "    torch.save(model.state_dict(), path_res+'/modelo_'+str(epoch)+'.pt')\n",
        "    \n",
        "    resultados = pd.DataFrame(list(zip(total_train_losses, total_val_losses, total_train_dice, total_val_dice,total_train_accuracy, total_val_accuracy)),\n",
        "                          columns = ['train_loss', 'val_loss', 'train_dice', 'test_dice', 'train_accuracy','val_accuracy'])\n",
        "    resultados.to_csv(path_res+'/'+'res_table.csv')"
      ],
      "metadata": {
        "id": "6_dgbeocj69H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(list(range(len(total_train_losses))), total_train_losses)\n",
        "plt.plot(list(range(len(total_train_losses))), total_val_losses)\n",
        "plt.legend(['Training loss', 'Validation loss'])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C_FvxYFTpZZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Format for submission"
      ],
      "metadata": {
        "id": "AFoMSWfwp2Es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto/1_Primeros_resultados_PSPNet/PSPNet_res101_368_1.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAvFJtywp9ER",
        "outputId": "e03b2476-8e4d-487a-950b-3d414043696c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test images\n",
        "test_list = sorted(glob.glob('/content/drive/MyDrive/Colab Notebooks/Deep Learning/Proyecto/TheTensors/test_images/*'))\n",
        "torch_list = [torch.load(i) for i in test_list]"
      ],
      "metadata": {
        "id": "USTkP9y8p9mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# calculate output for each image in test set and save the prediction in new folder\n",
        "for i in tqdm(range(len(torch_list))):\n",
        "  img_id = test_list[i].split('/')[-1].split('.')[0]\n",
        "  img = torch_list[i].unsqueeze(0)\n",
        "  output = model(img)\n",
        "  output = torch.argmax(output, dim=1).squeeze(0)\n",
        "  output = np.uint8(output)\n",
        "  output = Image.fromarray(output)\n",
        "  output.save(f'/content/drive/MyDrive/Colab Notebooks/Deep Learning/Final/{img_id}.png')"
      ],
      "metadata": {
        "id": "vJHAWF_FuEh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "tar = tarfile.open(\"submission.tar\", \"w\")\n",
        "\n",
        "for root, dir, files in os.walk('/content/drive/MyDrive/Colab Notebooks/Deep Learning/Final'):\n",
        "    for  file in files:\n",
        "        fullpath = os.path.join(root, file)\n",
        "        tar.add(fullpath, arcname=file)\n",
        "\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "ZCRK1v2juEUI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}